{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NPLearner import NPLearner\n",
    "from NPLearner import default_feature_func\n",
    "from NPLearner import NLTK_Model\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.classify import MaxentClassifier, \\\n",
    "                        ConditionalExponentialClassifier,\\\n",
    "                        DecisionTreeClassifier, \\\n",
    "                        NaiveBayesClassifier, \\\n",
    "                        WekaClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "IOB_LABEL_MAP = {\"O\": 0, \"B-NP\": 1, \"I-NP\": 2}\n",
    "IO_LABEL_MAP = {\"O\": 0, \"I-NP\": 1}\n",
    "\n",
    "PTB = \"treebank/\"\n",
    "RANDOM_SEED = 42 # Arbitrarily set so that the training and testing split is consistent accross all experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## traditional Machine Learning approaches\n",
    "\n",
    "MaxEnt1 = NLTK_Model(MaxentClassifier, \"MaximumEntropyClass_default\") ## automatically setting max_iter = 100\n",
    "CondExp = NLTK_Model(ConditionalExponentialClassifier, \"ConditionalExponentialClassifier_default\")\n",
    "DecTree = NLTK_Model(DecisionTreeClassifier, \"DecisionTreeClassifier_default\")\n",
    "NB = NLTK_Model(NaiveBayesClassifier, \"NaiveBayesClassifier_default\")\n",
    "\n",
    "## putting experiments in a list\n",
    "mods = [MaxEnt1, CondExp, DecTree, NB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TRAINING ----------\n",
      "Training MaximumEntropyClass_default...\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.561\n",
      "             2          -0.38307        0.835\n",
      "             3          -0.30433        0.871\n",
      "             4          -0.26866        0.890\n",
      "             5          -0.24637        0.901\n",
      "             6          -0.23030        0.909\n",
      "             7          -0.21778        0.916\n",
      "             8          -0.20756        0.920\n",
      "             9          -0.19896        0.925\n",
      "            10          -0.19155        0.929\n",
      "            11          -0.18508        0.931\n",
      "            12          -0.17935        0.934\n",
      "            13          -0.17422        0.936\n",
      "            14          -0.16959        0.939\n",
      "            15          -0.16539        0.941\n",
      "            16          -0.16155        0.942\n",
      "            17          -0.15803        0.944\n",
      "            18          -0.15477        0.945\n",
      "            19          -0.15176        0.946\n",
      "            20          -0.14895        0.947\n",
      "            21          -0.14634        0.947\n",
      "            22          -0.14389        0.948\n",
      "            23          -0.14160        0.949\n",
      "            24          -0.13944        0.949\n",
      "            25          -0.13740        0.950\n",
      "            26          -0.13548        0.951\n",
      "            27          -0.13366        0.951\n",
      "            28          -0.13193        0.951\n",
      "            29          -0.13029        0.952\n",
      "            30          -0.12873        0.952\n",
      "            31          -0.12724        0.952\n",
      "            32          -0.12582        0.953\n",
      "            33          -0.12447        0.953\n",
      "            34          -0.12317        0.953\n",
      "            35          -0.12193        0.954\n",
      "            36          -0.12073        0.954\n",
      "            37          -0.11959        0.955\n",
      "            38          -0.11849        0.955\n",
      "            39          -0.11743        0.955\n",
      "            40          -0.11642        0.955\n",
      "            41          -0.11544        0.956\n",
      "            42          -0.11449        0.956\n",
      "            43          -0.11358        0.956\n",
      "            44          -0.11269        0.956\n",
      "            45          -0.11184        0.956\n",
      "            46          -0.11102        0.956\n",
      "            47          -0.11022        0.957\n",
      "            48          -0.10945        0.957\n",
      "            49          -0.10870        0.957\n",
      "            50          -0.10797        0.957\n",
      "            51          -0.10727        0.957\n",
      "            52          -0.10659        0.957\n",
      "            53          -0.10592        0.958\n",
      "            54          -0.10528        0.958\n",
      "            55          -0.10465        0.958\n",
      "            56          -0.10404        0.958\n",
      "            57          -0.10345        0.958\n",
      "            58          -0.10287        0.958\n",
      "            59          -0.10231        0.958\n",
      "            60          -0.10176        0.958\n",
      "            61          -0.10122        0.959\n",
      "            62          -0.10070        0.959\n",
      "            63          -0.10019        0.959\n",
      "            64          -0.09970        0.959\n",
      "            65          -0.09921        0.959\n",
      "            66          -0.09874        0.959\n",
      "            67          -0.09828        0.959\n",
      "            68          -0.09783        0.959\n",
      "            69          -0.09739        0.959\n",
      "            70          -0.09696        0.959\n",
      "            71          -0.09653        0.959\n",
      "            72          -0.09612        0.960\n",
      "            73          -0.09572        0.960\n",
      "            74          -0.09532        0.960\n",
      "            75          -0.09493        0.960\n",
      "            76          -0.09456        0.960\n",
      "            77          -0.09418        0.960\n",
      "            78          -0.09382        0.960\n",
      "            79          -0.09346        0.960\n",
      "            80          -0.09311        0.960\n",
      "            81          -0.09277        0.960\n",
      "            82          -0.09243        0.960\n",
      "            83          -0.09210        0.960\n",
      "            84          -0.09178        0.960\n",
      "            85          -0.09146        0.960\n",
      "            86          -0.09115        0.961\n",
      "            87          -0.09085        0.961\n",
      "            88          -0.09054        0.961\n",
      "            89          -0.09025        0.961\n",
      "            90          -0.08996        0.961\n",
      "            91          -0.08967        0.961\n",
      "            92          -0.08939        0.961\n",
      "            93          -0.08912        0.961\n",
      "            94          -0.08885        0.961\n",
      "            95          -0.08858        0.961\n",
      "            96          -0.08832        0.961\n",
      "            97          -0.08806        0.961\n",
      "            98          -0.08781        0.961\n",
      "            99          -0.08756        0.961\n",
      "         Final          -0.08731        0.961\n",
      "Finished.\n",
      "\n",
      "Training ConditionalExponentialClassifier_default...\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.561\n",
      "             2          -0.38307        0.835\n",
      "             3          -0.30433        0.871\n",
      "             4          -0.26866        0.890\n",
      "             5          -0.24637        0.901\n",
      "             6          -0.23030        0.909\n",
      "             7          -0.21778        0.916\n",
      "             8          -0.20756        0.920\n",
      "             9          -0.19896        0.925\n",
      "            10          -0.19155        0.929\n",
      "            11          -0.18508        0.931\n",
      "            12          -0.17935        0.934\n",
      "            13          -0.17422        0.936\n",
      "            14          -0.16959        0.939\n",
      "            15          -0.16539        0.941\n",
      "            16          -0.16155        0.942\n",
      "            17          -0.15803        0.944\n",
      "            18          -0.15477        0.945\n",
      "            19          -0.15176        0.946\n",
      "            20          -0.14895        0.947\n",
      "            21          -0.14634        0.947\n",
      "            22          -0.14389        0.948\n",
      "            23          -0.14160        0.949\n",
      "            24          -0.13944        0.949\n",
      "            25          -0.13740        0.950\n",
      "            26          -0.13548        0.951\n",
      "            27          -0.13366        0.951\n",
      "            28          -0.13193        0.951\n",
      "            29          -0.13029        0.952\n",
      "            30          -0.12873        0.952\n",
      "            31          -0.12724        0.952\n",
      "            32          -0.12582        0.953\n",
      "            33          -0.12447        0.953\n",
      "            34          -0.12317        0.953\n",
      "            35          -0.12193        0.954\n",
      "            36          -0.12073        0.954\n",
      "            37          -0.11959        0.955\n",
      "            38          -0.11849        0.955\n",
      "            39          -0.11743        0.955\n",
      "            40          -0.11642        0.955\n",
      "            41          -0.11544        0.956\n",
      "            42          -0.11449        0.956\n",
      "            43          -0.11358        0.956\n",
      "            44          -0.11269        0.956\n",
      "            45          -0.11184        0.956\n",
      "            46          -0.11102        0.956\n",
      "            47          -0.11022        0.957\n",
      "            48          -0.10945        0.957\n",
      "            49          -0.10870        0.957\n",
      "            50          -0.10797        0.957\n",
      "            51          -0.10727        0.957\n",
      "            52          -0.10659        0.957\n",
      "            53          -0.10592        0.958\n",
      "            54          -0.10528        0.958\n",
      "            55          -0.10465        0.958\n",
      "            56          -0.10404        0.958\n",
      "            57          -0.10345        0.958\n",
      "            58          -0.10287        0.958\n",
      "            59          -0.10231        0.958\n",
      "            60          -0.10176        0.958\n",
      "            61          -0.10122        0.959\n",
      "            62          -0.10070        0.959\n",
      "            63          -0.10019        0.959\n",
      "            64          -0.09970        0.959\n",
      "            65          -0.09921        0.959\n",
      "            66          -0.09874        0.959\n",
      "            67          -0.09828        0.959\n",
      "            68          -0.09783        0.959\n",
      "            69          -0.09739        0.959\n",
      "            70          -0.09696        0.959\n",
      "            71          -0.09653        0.959\n",
      "            72          -0.09612        0.960\n",
      "            73          -0.09572        0.960\n",
      "            74          -0.09532        0.960\n",
      "            75          -0.09493        0.960\n",
      "            76          -0.09456        0.960\n",
      "            77          -0.09418        0.960\n",
      "            78          -0.09382        0.960\n",
      "            79          -0.09346        0.960\n",
      "            80          -0.09311        0.960\n",
      "            81          -0.09277        0.960\n",
      "            82          -0.09243        0.960\n",
      "            83          -0.09210        0.960\n",
      "            84          -0.09178        0.960\n",
      "            85          -0.09146        0.960\n",
      "            86          -0.09115        0.961\n",
      "            87          -0.09085        0.961\n",
      "            88          -0.09054        0.961\n",
      "            89          -0.09025        0.961\n",
      "            90          -0.08996        0.961\n",
      "            91          -0.08967        0.961\n",
      "            92          -0.08939        0.961\n",
      "            93          -0.08912        0.961\n",
      "            94          -0.08885        0.961\n",
      "            95          -0.08858        0.961\n",
      "            96          -0.08832        0.961\n",
      "            97          -0.08806        0.961\n",
      "            98          -0.08781        0.961\n",
      "            99          -0.08756        0.961\n",
      "         Final          -0.08731        0.961\n",
      "Finished.\n",
      "\n",
      "Training DecisionTreeClassifier_default...\n",
      "Finished.\n",
      "\n",
      "Training NaiveBayesClassifier_default...\n",
      "Finished.\n",
      "\n",
      "---------- PREDICTING ----------\n",
      "Predicting for test data for MaximumEntropyClass_default\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for ConditionalExponentialClassifier_default\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for DecisionTreeClassifier_default\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for NaiveBayesClassifier_default\n",
      "Finished. \n",
      "\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: MaximumEntropyClass_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.869320295477\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[4832   38  614]\n",
      " [  40 2051  629]\n",
      " [ 710  428 9475]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: ConditionalExponentialClassifier_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.869320295477\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[4832   38  614]\n",
      " [  40 2051  629]\n",
      " [ 710  428 9475]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: DecisionTreeClassifier_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.80937450178\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[4275  102 1107]\n",
      " [  54 1864  802]\n",
      " [ 840  682 9091]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: NaiveBayesClassifier_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.82069405325\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[5176   48  260]\n",
      " [  60 2401  259]\n",
      " [1524 1223 7866]]\n"
     ]
    }
   ],
   "source": [
    "# Max = 100 iterations\n",
    "IOB_experiment = NPLearner(PTB, mods, default_feature_func, verbose=True, random_state=RANDOM_SEED)\n",
    "\n",
    "IOB_experiment.fit()\n",
    "IOB_experiment.predict()\n",
    "\n",
    "IOB_metrics_default = IOB_experiment.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TRAINING ----------\n",
      "Training MaximumEntropyClass_default...\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.709\n",
      "             2          -0.22021        0.894\n",
      "             3          -0.17594        0.918\n",
      "             4          -0.15610        0.927\n",
      "             5          -0.14376        0.933\n",
      "             6          -0.13493        0.938\n",
      "             7          -0.12810        0.942\n",
      "             8          -0.12256        0.945\n",
      "             9          -0.11790        0.948\n",
      "            10          -0.11390        0.950\n",
      "            11          -0.11041        0.952\n",
      "            12          -0.10732        0.954\n",
      "            13          -0.10455        0.956\n",
      "            14          -0.10204        0.957\n",
      "            15          -0.09977        0.958\n",
      "            16          -0.09769        0.959\n",
      "            17          -0.09578        0.960\n",
      "            18          -0.09401        0.961\n",
      "            19          -0.09237        0.962\n",
      "            20          -0.09084        0.963\n",
      "            21          -0.08941        0.963\n",
      "            22          -0.08807        0.964\n",
      "            23          -0.08681        0.964\n",
      "            24          -0.08563        0.964\n",
      "            25          -0.08451        0.965\n",
      "            26          -0.08345        0.965\n",
      "            27          -0.08245        0.965\n",
      "            28          -0.08149        0.966\n",
      "            29          -0.08058        0.966\n",
      "            30          -0.07972        0.966\n",
      "            31          -0.07889        0.967\n",
      "            32          -0.07811        0.967\n",
      "            33          -0.07735        0.967\n",
      "            34          -0.07663        0.967\n",
      "            35          -0.07594        0.968\n",
      "            36          -0.07527        0.968\n",
      "            37          -0.07463        0.968\n",
      "            38          -0.07402        0.968\n",
      "            39          -0.07342        0.968\n",
      "            40          -0.07285        0.968\n",
      "            41          -0.07230        0.969\n",
      "            42          -0.07177        0.969\n",
      "            43          -0.07126        0.969\n",
      "            44          -0.07076        0.969\n",
      "            45          -0.07028        0.969\n",
      "            46          -0.06982        0.969\n",
      "            47          -0.06937        0.969\n",
      "            48          -0.06893        0.969\n",
      "            49          -0.06851        0.969\n",
      "            50          -0.06810        0.970\n",
      "            51          -0.06770        0.970\n",
      "            52          -0.06731        0.970\n",
      "            53          -0.06694        0.970\n",
      "            54          -0.06657        0.970\n",
      "            55          -0.06622        0.970\n",
      "            56          -0.06587        0.970\n",
      "            57          -0.06553        0.970\n",
      "            58          -0.06521        0.971\n",
      "            59          -0.06489        0.971\n",
      "            60          -0.06457        0.971\n",
      "            61          -0.06427        0.971\n",
      "            62          -0.06398        0.971\n",
      "            63          -0.06369        0.971\n",
      "            64          -0.06340        0.971\n",
      "            65          -0.06313        0.971\n",
      "            66          -0.06286        0.971\n",
      "            67          -0.06260        0.971\n",
      "            68          -0.06234        0.971\n",
      "            69          -0.06209        0.971\n",
      "            70          -0.06184        0.971\n",
      "            71          -0.06160        0.971\n",
      "            72          -0.06137        0.971\n",
      "            73          -0.06114        0.971\n",
      "            74          -0.06091        0.972\n",
      "            75          -0.06069        0.972\n",
      "            76          -0.06048        0.972\n",
      "            77          -0.06026        0.972\n",
      "            78          -0.06006        0.972\n",
      "            79          -0.05985        0.972\n",
      "            80          -0.05965        0.972\n",
      "            81          -0.05946        0.972\n",
      "            82          -0.05927        0.972\n",
      "            83          -0.05908        0.972\n",
      "            84          -0.05889        0.972\n",
      "            85          -0.05871        0.972\n",
      "            86          -0.05853        0.972\n",
      "            87          -0.05836        0.972\n",
      "            88          -0.05819        0.972\n",
      "            89          -0.05802        0.972\n",
      "            90          -0.05785        0.972\n",
      "            91          -0.05769        0.972\n",
      "            92          -0.05753        0.972\n",
      "            93          -0.05737        0.972\n",
      "            94          -0.05722        0.972\n",
      "            95          -0.05707        0.972\n",
      "            96          -0.05692        0.972\n",
      "            97          -0.05677        0.972\n",
      "            98          -0.05663        0.972\n",
      "            99          -0.05648        0.972\n",
      "         Final          -0.05634        0.972\n",
      "Finished.\n",
      "\n",
      "Training ConditionalExponentialClassifier_default...\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.709\n",
      "             2          -0.22021        0.894\n",
      "             3          -0.17594        0.918\n",
      "             4          -0.15610        0.927\n",
      "             5          -0.14376        0.933\n",
      "             6          -0.13493        0.938\n",
      "             7          -0.12810        0.942\n",
      "             8          -0.12256        0.945\n",
      "             9          -0.11790        0.948\n",
      "            10          -0.11390        0.950\n",
      "            11          -0.11041        0.952\n",
      "            12          -0.10732        0.954\n",
      "            13          -0.10455        0.956\n",
      "            14          -0.10204        0.957\n",
      "            15          -0.09977        0.958\n",
      "            16          -0.09769        0.959\n",
      "            17          -0.09578        0.960\n",
      "            18          -0.09401        0.961\n",
      "            19          -0.09237        0.962\n",
      "            20          -0.09084        0.963\n",
      "            21          -0.08941        0.963\n",
      "            22          -0.08807        0.964\n",
      "            23          -0.08681        0.964\n",
      "            24          -0.08563        0.964\n",
      "            25          -0.08451        0.965\n",
      "            26          -0.08345        0.965\n",
      "            27          -0.08245        0.965\n",
      "            28          -0.08149        0.966\n",
      "            29          -0.08058        0.966\n",
      "            30          -0.07972        0.966\n",
      "            31          -0.07889        0.967\n",
      "            32          -0.07811        0.967\n",
      "            33          -0.07735        0.967\n",
      "            34          -0.07663        0.967\n",
      "            35          -0.07594        0.968\n",
      "            36          -0.07527        0.968\n",
      "            37          -0.07463        0.968\n",
      "            38          -0.07402        0.968\n",
      "            39          -0.07342        0.968\n",
      "            40          -0.07285        0.968\n",
      "            41          -0.07230        0.969\n",
      "            42          -0.07177        0.969\n",
      "            43          -0.07126        0.969\n",
      "            44          -0.07076        0.969\n",
      "            45          -0.07028        0.969\n",
      "            46          -0.06982        0.969\n",
      "            47          -0.06937        0.969\n",
      "            48          -0.06893        0.969\n",
      "            49          -0.06851        0.969\n",
      "            50          -0.06810        0.970\n",
      "            51          -0.06770        0.970\n",
      "            52          -0.06731        0.970\n",
      "            53          -0.06694        0.970\n",
      "            54          -0.06657        0.970\n",
      "            55          -0.06622        0.970\n",
      "            56          -0.06587        0.970\n",
      "            57          -0.06553        0.970\n",
      "            58          -0.06521        0.971\n",
      "            59          -0.06489        0.971\n",
      "            60          -0.06457        0.971\n",
      "            61          -0.06427        0.971\n",
      "            62          -0.06398        0.971\n",
      "            63          -0.06369        0.971\n",
      "            64          -0.06340        0.971\n",
      "            65          -0.06313        0.971\n",
      "            66          -0.06286        0.971\n",
      "            67          -0.06260        0.971\n",
      "            68          -0.06234        0.971\n",
      "            69          -0.06209        0.971\n",
      "            70          -0.06184        0.971\n",
      "            71          -0.06160        0.971\n",
      "            72          -0.06137        0.971\n",
      "            73          -0.06114        0.971\n",
      "            74          -0.06091        0.972\n",
      "            75          -0.06069        0.972\n",
      "            76          -0.06048        0.972\n",
      "            77          -0.06026        0.972\n",
      "            78          -0.06006        0.972\n",
      "            79          -0.05985        0.972\n",
      "            80          -0.05965        0.972\n",
      "            81          -0.05946        0.972\n",
      "            82          -0.05927        0.972\n",
      "            83          -0.05908        0.972\n",
      "            84          -0.05889        0.972\n",
      "            85          -0.05871        0.972\n",
      "            86          -0.05853        0.972\n",
      "            87          -0.05836        0.972\n",
      "            88          -0.05819        0.972\n",
      "            89          -0.05802        0.972\n",
      "            90          -0.05785        0.972\n",
      "            91          -0.05769        0.972\n",
      "            92          -0.05753        0.972\n",
      "            93          -0.05737        0.972\n",
      "            94          -0.05722        0.972\n",
      "            95          -0.05707        0.972\n",
      "            96          -0.05692        0.972\n",
      "            97          -0.05677        0.972\n",
      "            98          -0.05663        0.972\n",
      "            99          -0.05648        0.972\n",
      "         Final          -0.05634        0.972\n",
      "Finished.\n",
      "\n",
      "Training DecisionTreeClassifier_default...\n",
      "Finished.\n",
      "\n",
      "Training NaiveBayesClassifier_default...\n",
      "Finished.\n",
      "\n",
      "---------- PREDICTING ----------\n",
      "Predicting for test data for MaximumEntropyClass_default\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for ConditionalExponentialClassifier_default\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for DecisionTreeClassifier_default\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for NaiveBayesClassifier_default\n",
      "Finished. \n",
      "\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: MaximumEntropyClass_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.921985438699\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[ 4807   677]\n",
      " [  791 12542]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: ConditionalExponentialClassifier_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.921985438699\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[ 4807   677]\n",
      " [  791 12542]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: DecisionTreeClassifier_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.888505075198\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[ 4284  1200]\n",
      " [  898 12435]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: NaiveBayesClassifier_default \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.889195939842\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[ 5246   238]\n",
      " [ 1847 11486]]\n"
     ]
    }
   ],
   "source": [
    "## experiment with IO labeling instead of IOB -- all accuracies are expected to be higher\n",
    "\n",
    "IO_experiment = NPLearner(PTB, mods, default_feature_func, label_map=IO_LABEL_MAP, \n",
    "                        NP_tagging_type=\"IO\", verbose=True, random_state=RANDOM_SEED)\n",
    "\n",
    "IO_experiment.fit()\n",
    "IO_experiment.predict()\n",
    "\n",
    "IO_metrics_default = IO_experiment.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For IOB labeling:\n",
      "MaximumEntropyClass_default:0.8693202954774938\n",
      "ConditionalExponentialClassifier_default:0.8693202954774938\n",
      "DecisionTreeClassifier_default:0.809374501780305\n",
      "NaiveBayesClassifier_default:0.820694053249721\n",
      "For IO labeling:\n",
      "MaximumEntropyClass_default:0.9219854386990487\n",
      "ConditionalExponentialClassifier_default:0.9219854386990487\n",
      "DecisionTreeClassifier_default:0.8885050751979593\n",
      "NaiveBayesClassifier_default:0.8891959398416326\n"
     ]
    }
   ],
   "source": [
    "## Displaying tabulated metrics_default for IOB labeling:\n",
    "\n",
    "print(\"For IOB labeling:\")\n",
    "\n",
    "for mod_dic in IOB_metrics_default:\n",
    "    ## format is [ModelName]: [Accuracy]\n",
    "    print(\"{}:{}\".format(mod_dic[\"Model type\"], mod_dic[\"Accuracy score\"]))\n",
    "\n",
    "print(\"For IO labeling:\")\n",
    "\n",
    "for mod_dic in IO_metrics_default:\n",
    "    ## format is [ModelName]: [Accuracy]\n",
    "    print(\"{}:{}\".format(mod_dic[\"Model type\"], mod_dic[\"Accuracy score\"]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
