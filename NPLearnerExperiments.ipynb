{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NPLearner import NPLearner\n",
    "from NPLearner import default_feature_func\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.classify import MaxentClassifier, \\\n",
    "                        ConditionalExponentialClassifier,\\\n",
    "                        DecisionTreeClassifier, \\\n",
    "                        NaiveBayesClassifier, \\\n",
    "                        WekaClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PTB = \"treebank/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining experiments\n",
    "\n",
    "subjects = {\"Decision Tree Classifier\": DecisionTreeClassifier,\n",
    "            \"Maximum Entorpy Classifier\": MaxentClassifier,\n",
    "            \"Naive Bayes Classifier\": NaiveBayesClassifier,\n",
    "            \"Conditional Exponential Classifier\": ConditionalExponentialClassifier}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TRAINING ----------\n",
      "Training Decision Tree Classifier...\n",
      "Finished.\n",
      "\n",
      "Training Maximum Entorpy Classifier...\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.562\n",
      "             2          -0.38053        0.836\n",
      "             3          -0.30256        0.871\n",
      "             4          -0.26721        0.889\n",
      "             5          -0.24510        0.901\n",
      "             6          -0.22916        0.908\n",
      "             7          -0.21674        0.915\n",
      "             8          -0.20661        0.920\n",
      "             9          -0.19807        0.924\n",
      "            10          -0.19072        0.929\n",
      "            11          -0.18430        0.932\n",
      "            12          -0.17860        0.935\n",
      "            13          -0.17351        0.937\n",
      "            14          -0.16891        0.939\n",
      "            15          -0.16473        0.941\n",
      "            16          -0.16091        0.943\n",
      "            17          -0.15740        0.944\n",
      "            18          -0.15417        0.945\n",
      "            19          -0.15116        0.946\n",
      "            20          -0.14837        0.947\n",
      "            21          -0.14577        0.948\n",
      "            22          -0.14333        0.949\n",
      "            23          -0.14104        0.949\n",
      "            24          -0.13889        0.950\n",
      "            25          -0.13686        0.951\n",
      "            26          -0.13494        0.951\n",
      "            27          -0.13312        0.952\n",
      "            28          -0.13140        0.952\n",
      "            29          -0.12977        0.952\n",
      "            30          -0.12821        0.953\n",
      "            31          -0.12673        0.953\n",
      "            32          -0.12531        0.954\n",
      "            33          -0.12396        0.954\n",
      "            34          -0.12267        0.954\n",
      "            35          -0.12143        0.954\n",
      "            36          -0.12024        0.955\n",
      "            37          -0.11910        0.955\n",
      "            38          -0.11800        0.955\n",
      "            39          -0.11695        0.956\n",
      "            40          -0.11593        0.956\n",
      "            41          -0.11496        0.956\n",
      "            42          -0.11401        0.956\n",
      "            43          -0.11310        0.956\n",
      "            44          -0.11223        0.957\n",
      "            45          -0.11138        0.957\n",
      "            46          -0.11055        0.957\n",
      "            47          -0.10976        0.957\n",
      "            48          -0.10899        0.957\n",
      "            49          -0.10824        0.957\n",
      "            50          -0.10752        0.958\n",
      "            51          -0.10682        0.958\n",
      "            52          -0.10614        0.958\n",
      "            53          -0.10548        0.958\n",
      "            54          -0.10483        0.958\n",
      "            55          -0.10421        0.958\n",
      "            56          -0.10360        0.958\n",
      "            57          -0.10301        0.958\n",
      "            58          -0.10244        0.959\n",
      "            59          -0.10188        0.959\n",
      "            60          -0.10133        0.959\n",
      "            61          -0.10080        0.959\n",
      "            62          -0.10028        0.959\n",
      "            63          -0.09977        0.959\n",
      "            64          -0.09928        0.959\n",
      "            65          -0.09880        0.959\n",
      "            66          -0.09833        0.959\n",
      "            67          -0.09787        0.959\n",
      "            68          -0.09742        0.959\n",
      "            69          -0.09698        0.959\n",
      "            70          -0.09655        0.960\n",
      "            71          -0.09613        0.960\n",
      "            72          -0.09572        0.960\n",
      "            73          -0.09532        0.960\n",
      "            74          -0.09492        0.960\n",
      "            75          -0.09454        0.960\n",
      "            76          -0.09416        0.960\n",
      "            77          -0.09379        0.960\n",
      "            78          -0.09343        0.960\n",
      "            79          -0.09308        0.960\n",
      "            80          -0.09273        0.960\n",
      "            81          -0.09239        0.960\n",
      "            82          -0.09205        0.960\n",
      "            83          -0.09173        0.960\n",
      "            84          -0.09140        0.960\n",
      "            85          -0.09109        0.961\n",
      "            86          -0.09078        0.961\n",
      "            87          -0.09047        0.961\n",
      "            88          -0.09017        0.961\n",
      "            89          -0.08988        0.961\n",
      "            90          -0.08959        0.961\n",
      "            91          -0.08931        0.961\n",
      "            92          -0.08903        0.961\n",
      "            93          -0.08876        0.961\n",
      "            94          -0.08849        0.961\n",
      "            95          -0.08822        0.961\n",
      "            96          -0.08796        0.961\n",
      "            97          -0.08771        0.961\n",
      "            98          -0.08745        0.961\n",
      "            99          -0.08721        0.961\n",
      "         Final          -0.08696        0.961\n",
      "Finished.\n",
      "\n",
      "Training Naive Bayes Classifier...\n",
      "Finished.\n",
      "\n",
      "Training Conditional Exponential Classifier...\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.562\n",
      "             2          -0.38053        0.836\n",
      "             3          -0.30256        0.871\n",
      "             4          -0.26721        0.889\n",
      "             5          -0.24510        0.901\n",
      "             6          -0.22916        0.908\n",
      "             7          -0.21674        0.915\n",
      "             8          -0.20661        0.920\n",
      "             9          -0.19807        0.924\n",
      "            10          -0.19072        0.929\n",
      "            11          -0.18430        0.932\n",
      "            12          -0.17860        0.935\n",
      "            13          -0.17351        0.937\n",
      "            14          -0.16891        0.939\n",
      "            15          -0.16473        0.941\n",
      "            16          -0.16091        0.943\n",
      "            17          -0.15740        0.944\n",
      "            18          -0.15417        0.945\n",
      "            19          -0.15116        0.946\n",
      "            20          -0.14837        0.947\n",
      "            21          -0.14577        0.948\n",
      "            22          -0.14333        0.949\n",
      "            23          -0.14104        0.949\n",
      "            24          -0.13889        0.950\n",
      "            25          -0.13686        0.951\n",
      "            26          -0.13494        0.951\n",
      "            27          -0.13312        0.952\n",
      "            28          -0.13140        0.952\n",
      "            29          -0.12977        0.952\n",
      "            30          -0.12821        0.953\n",
      "            31          -0.12673        0.953\n",
      "            32          -0.12531        0.954\n",
      "            33          -0.12396        0.954\n",
      "            34          -0.12267        0.954\n",
      "            35          -0.12143        0.954\n",
      "            36          -0.12024        0.955\n",
      "            37          -0.11910        0.955\n",
      "            38          -0.11800        0.955\n",
      "            39          -0.11695        0.956\n",
      "            40          -0.11593        0.956\n",
      "            41          -0.11496        0.956\n",
      "            42          -0.11401        0.956\n",
      "            43          -0.11310        0.956\n",
      "            44          -0.11223        0.957\n",
      "            45          -0.11138        0.957\n",
      "            46          -0.11055        0.957\n",
      "            47          -0.10976        0.957\n",
      "            48          -0.10899        0.957\n",
      "            49          -0.10824        0.957\n",
      "            50          -0.10752        0.958\n",
      "            51          -0.10682        0.958\n",
      "            52          -0.10614        0.958\n",
      "            53          -0.10548        0.958\n",
      "            54          -0.10483        0.958\n",
      "            55          -0.10421        0.958\n",
      "            56          -0.10360        0.958\n",
      "            57          -0.10301        0.958\n",
      "            58          -0.10244        0.959\n",
      "            59          -0.10188        0.959\n",
      "            60          -0.10133        0.959\n",
      "            61          -0.10080        0.959\n",
      "            62          -0.10028        0.959\n",
      "            63          -0.09977        0.959\n",
      "            64          -0.09928        0.959\n",
      "            65          -0.09880        0.959\n",
      "            66          -0.09833        0.959\n",
      "            67          -0.09787        0.959\n",
      "            68          -0.09742        0.959\n",
      "            69          -0.09698        0.959\n",
      "            70          -0.09655        0.960\n",
      "            71          -0.09613        0.960\n",
      "            72          -0.09572        0.960\n",
      "            73          -0.09532        0.960\n",
      "            74          -0.09492        0.960\n",
      "            75          -0.09454        0.960\n",
      "            76          -0.09416        0.960\n",
      "            77          -0.09379        0.960\n",
      "            78          -0.09343        0.960\n",
      "            79          -0.09308        0.960\n",
      "            80          -0.09273        0.960\n",
      "            81          -0.09239        0.960\n",
      "            82          -0.09205        0.960\n",
      "            83          -0.09173        0.960\n",
      "            84          -0.09140        0.960\n",
      "            85          -0.09109        0.961\n",
      "            86          -0.09078        0.961\n",
      "            87          -0.09047        0.961\n",
      "            88          -0.09017        0.961\n",
      "            89          -0.08988        0.961\n",
      "            90          -0.08959        0.961\n",
      "            91          -0.08931        0.961\n",
      "            92          -0.08903        0.961\n",
      "            93          -0.08876        0.961\n",
      "            94          -0.08849        0.961\n",
      "            95          -0.08822        0.961\n",
      "            96          -0.08796        0.961\n",
      "            97          -0.08771        0.961\n",
      "            98          -0.08745        0.961\n",
      "            99          -0.08721        0.961\n",
      "         Final          -0.08696        0.961\n",
      "Finished.\n",
      "\n",
      "---------- PREDICTING ----------\n",
      "Predicting for test data for Decision Tree Classifier\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for Maximum Entorpy Classifier\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for Naive Bayes Classifier\n",
      "Finished. \n",
      "\n",
      "Predicting for test data for Conditional Exponential Classifier\n",
      "Finished. \n",
      "\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: Decision Tree Classifier \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.810968804804\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[4403   52 1050]\n",
      " [ 113 1732  973]\n",
      " [ 938  431 9125]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: Maximum Entorpy Classifier \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.867247701546\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[4813   33  659]\n",
      " [  44 2096  678]\n",
      " [ 680  404 9410]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: Naive Bayes Classifier \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.819312323962\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[5160   43  302]\n",
      " [  60 2472  286]\n",
      " [1516 1193 7785]]\n",
      "---------- EVALUATING ----------\n",
      "\n",
      "\n",
      "For model: Conditional Exponential Classifier \n",
      "-------------------\n",
      "Accuracy \n",
      "-----------------\n",
      "0.867247701546\n",
      "\n",
      "\n",
      "Confusion Matrix \n",
      "------------------\n",
      "[[4813   33  659]\n",
      " [  44 2096  678]\n",
      " [ 680  404 9410]]\n"
     ]
    }
   ],
   "source": [
    "# Max = 100 iterations\n",
    "experiment = NPLearner(PTB, subjects, default_feature_func, verbose=True, max_iter=100)\n",
    "\n",
    "experiment.fit()\n",
    "\n",
    "experiment.predict()\n",
    "\n",
    "metrics = experiment.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
